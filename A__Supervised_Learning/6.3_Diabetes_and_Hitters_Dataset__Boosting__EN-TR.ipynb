{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **- Machine Learning -**\n",
    "# _* **Boosting** with Python *_\n",
    "## **+ '``Gradient Boosting``' , '``Adaptive Boosting``' and ``'Extreme Gradient Booster'`` +**\n",
    "### **+ _Diabetes Data - Hitters Data_ +**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate (LAMBDA)** ile **kaç tane Ağaç (B)** kuracağımızı belirlememiz lazım... Bunlar bizim için birer Parametre... Bu Parametreleri belirlemek için de **Cross Validation** yapmamız lazım... Yani en ideal **Boosting**'i bulmamız lazım... Bunun için de **GridSearchCV** kullanmamız lazım... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``Classification`` with **Diabetes Data :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preg</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insul</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DPF</th>\n",
       "      <th>Age</th>\n",
       "      <th>Diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Preg  Glucose  BP  SkinThick  Insul   BMI    DPF  Age  Diabetes\n",
       "0       6      148  72         35      0  33.6  0.627   50         1\n",
       "1       1       85  66         29      0  26.6  0.351   31         0\n",
       "2       8      183  64          0      0  23.3  0.672   32         1\n",
       "3       1       89  66         23     94  28.1  0.167   21         0\n",
       "4       0      137  40         35    168  43.1  2.288   33         1\n",
       "..    ...      ...  ..        ...    ...   ...    ...  ...       ...\n",
       "763    10      101  76         48    180  32.9  0.171   63         0\n",
       "764     2      122  70         27      0  36.8  0.340   27         0\n",
       "765     5      121  72         23    112  26.2  0.245   30         0\n",
       "766     1      126  60          0      0  30.1  0.349   47         1\n",
       "767     1       93  70         31      0  30.4  0.315   23         0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('diabetesdata.txt')\n",
    "X = df.drop('Diabetes',axis=1)\n",
    "y = df['Diabetes']\n",
    "X_col = X.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Preg', 'Glucose', 'BP', 'SkinThick', 'Insul', 'BMI', 'DPF', 'Age'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets ::\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Boosting**\n",
    "### **``Classicifation``** with ``Gradient Boosting``\n",
    "#### ``GradientBoostingClassifier()`` modülümüzün içerisinde neler varmış bakalım **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ + DEFAULT olarak **Ağaç** oluşturuyor... Yani bir **Ağaç** FIT ettiriyor...\n",
    "+ + DEFAULT olarak **(max_depth=3)** Parametresi **:** Normalde **Decision Tree**'de Ağaç kurarken DEFAULT olarak **maksimum derinlik sınırı** yoktu... Ama **Gradient Boosting Classifier**'de bu var... Yani daha derin **Ağaçlar** yaratmıyor... Yani çok da iyi öğrenebilen **Ağaçlar** kurmuyor... Bunun **Ağaçları** biraz salak... Tabii bu değeri değiştirebiliriz...\n",
    "+ + DEFAULT olarak **(n_estimators=100)** Parametresi **:** Default olarak **'100'** tane **Ağaç ,** yani **STUMP** FIT ediyor...\n",
    "+ + DEFAULT olarak **(learning_rate=0.1)** Parametresi **:** Bu değer LAMBDA'dır... Ne kadar küçük olursa **,** modelin **her bir Ağacının** Tahmini o kadar küçülür **,** değersizleşir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Şimdi de bir **GradientBoostingClassifier()** modeli kuralım **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=1000 , max_depth=1 , learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki **Accuracy** değerine baktığımızda **,** çok da başarılı bir sonuç aldığımız söylenemez..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### There is a trade-off between number of trees and learning rate.\n",
    "#### **Ağaç sayısı** ile **öğrenme hızı (oranı)** arasında bir değiş tokuş vardır.\n",
    "#### You should never check the test performance when tuning parameters!!! This is only for illustrative purposes\n",
    "#### Parametreleri ayarlarken , asla test performansını kontrol etmemelisiniz!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Learning Rate**'yi biraz daha azaltalım. (DEFAULT hale getirelim) **:**\n",
    "<br> Yani biraz daha YAVAŞ öğrenelim..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=1000 , max_depth=1 , learning_rate=0.1)\n",
    "\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda gördüğümüz gibi **, Learning Rate**'yi azaltınca **Accuracy** değeri de anlamlı bir şekilde arttı..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Yukarıdaki iki örnekte Ağaç Sayısı yani iterasyon sayısı birbirleri ile AYNI olmasına rağmen **,** Öğrenme Hızı'nı **azaltınca** neden Accuracy değeri arttı **?** Veya Öğrenme Hızı'nı **arttırınıca** neden Accuracy değeri düştü **?**\n",
    "+ + AYNI sayıda Ağaç varken **,** Öğrenme Hızı'nı arttırınca OVERFIT etme ihtimalimiz de artar. Hızlıca her şeyi öğrenmeye çalıştığı için muhtemelen OVERFIT oldu ve onun için **Accuracy** değeri daha düşük çıktı..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Şimdi de **Learning Rate**'yi biraz daha düşürelim **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7337662337662337\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=1000 , max_depth=1 , learning_rate=0.01)\n",
    "\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda gördüğümüz gibi **, Öğrenme Hızı**'nı daha da azaltmamıza RAĞMEN BU DEFA **Accuracy** değeri biraz düştü... Şimdi de çok büyük ihtimalle UNDERFIT oldu. Yani o kadar yavaş öğrenmeye çalıştı ki **,** öğrenebileceği bazı şeyleri öğrenemedi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10000 , max_depth=1 , learning_rate=0.01)\n",
    "model.fit(X_train , y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**++++ CRITICAL NOTE ++++**\n",
    "<br> İdeali **;** olabildiğince yavaş öğrenip **,** bir o kadar da çok modele sormaktır... Yani **'learning_rate'**'yi olabildiğince azaltıp **, 'n_estimators'**'ü olabildiğince arttırmaktır...\n",
    "<br> Ama her zaman da böyle olacak diye bir şey yok... Onun için en ideal Parametreleri bir şekilde bulmamız gerelir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi de **,** ideal Parametreleri bulmaya çalışalım **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'learning_rate': 0.01, 'n_estimators': 5000}\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(max_depth=1)\n",
    "\n",
    "# Number of trees fit ::\n",
    "n_estimators = [100 , 1000 , 5000 , 10000]\n",
    "# Learning rate ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'n_estimators' : n_estimators ,\n",
    " 'learning_rate' : learning_rate ,\n",
    " }\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=model , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='accuracy' , n_jobs= -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıda modeli kurarken **GradientBoostingClassifier(max_depth=1)** şeklinde kurduk... Yani **maksimum derinlik** Parametresini direkt yazdık... Ama istersek onu da **,** en ideali aranan Parametreler arasına ekleyip o şekilde çalıştırabilirdik..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ideal Parametreleri **; \" { 'learning_rate': 0.01 , 'n_estimators': 5000 } \"** şeklinde buldu...\n",
    "+ Şimdi de bu Parametreleri yerine koyup modeli EN BAŞTAN kuralım ve Tahminleme yapıp Score'nine bakalım **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=5000 , max_depth=1 , learning_rate=0.01)\n",
    "\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adaptive Boosting**\n",
    "Veri Noktalarının Ağırlıklarını değiştirerek çalışıyor...\n",
    "Her STUMP'ta Yanlış Sınıfta kalan Veri Noktalarının Ağırlıklarını ARTTIRIYOR **,** Doğru Sınıfta kalan Veri Noktalarının da Ağırlıklarını AZALTIYOR... Bunu **,** her yeni STUMP'ta güncelleyerek devam ediyor...\n",
    "### **``AdaboostClassifier``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'learning_rate': 0.1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "n_estimators = [50 , 100 , 1000 , 5000]\n",
    "# Number of features at every split ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'n_estimators' : n_estimators ,\n",
    " 'learning_rate' : learning_rate ,\n",
    " }\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=model , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='accuracy' , n_jobs = -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100 , learning_rate=0.1)\n",
    "model.fit(X_train , y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:' , accuracy_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Booster (``XGBClassifier``)\n",
    "#### ``Extreme Gradient Booster:`` A popular library that implements boosting faster and generally provides better performance.\n",
    "#### ``Extreme Gradient Booster:`` Boosting'i daha hızlı uygulayan ve genellikle daha iyi performans sağlayan popüler bir Kütüphanedir...\n",
    "<br> **Scikit-Learn**'deki **Gredient Boosting Classifier**'in bir alternatifidir...\n",
    "<br> Genelde **Scikit-Learn**'den daha iyi bir performans gösterir...\n",
    "<br> Ayrıntılarını , arka plandaki çalışma şeklini ve optimizasyonlarını ayrıntılı bir şekilde okuyarak öğrenebiliriz...\n",
    "<br> Yine iyi çalışan başka Kütüphaneler de var... **``'LightGBM'``** Kütüphanesi de bunlardan biridir ve o da iyi çalışır..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None,\n",
       "              enable_categorical=False, gamma=None, gpu_id=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   pip install XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\ITUmindset\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:23:30] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'learning_rate': 0.01, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "\n",
    "n_estimators = [50 , 100 , 1000 , 5000]\n",
    "# Number of features at every split ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'n_estimators' : n_estimators ,\n",
    " 'learning_rate' : learning_rate ,\n",
    " }\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=model , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='accuracy' , n_jobs = -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:27:34] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators=100 , learning_rate=0.01)\n",
    "\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:' , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``Regression`` with **Hitters Data :**\n",
    "## **``Regression``** with ``Gradient Boosting``\n",
    "### **'GradientBoostingRegressor'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Öncelikle **, Hitters** Veri Setini çağıralım ve düzenleyelim **:**\n",
    "<br> **+--- NOTE ---> 'NaN'** değerleri Veri Setimizden direkt atmak yerine düzenlememiz daha doğru olacaktır... Ama burada uğraşmayacağız..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters_Data.csv')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "dummies = pd.get_dummies(df[['League' , 'Division' , 'NewLeague']])\n",
    "\n",
    "y = np.log(df.Salary)\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables ::\n",
    "X__ = df.drop(['Salary' , 'League' , 'Division' , 'NewLeague'] , axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X. ::\n",
    "X = pd.concat([X__ , dummies[['League_N', 'Division_W', 'NewLeague_N']]] , axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5628.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4408.0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>497.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2703.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>492.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5511.0</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>475.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>573.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>631.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4908.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat   CHits  CHmRun  \\\n",
       "1    315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0   835.0    69.0   \n",
       "2    479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0   457.0    63.0   \n",
       "3    496.0  141.0   20.0  65.0  78.0   37.0   11.0  5628.0  1575.0   225.0   \n",
       "4    321.0   87.0   10.0  39.0  42.0   30.0    2.0   396.0   101.0    12.0   \n",
       "5    594.0  169.0    4.0  74.0  51.0   35.0   11.0  4408.0  1133.0    19.0   \n",
       "..     ...    ...    ...   ...   ...    ...    ...     ...     ...     ...   \n",
       "317  497.0  127.0    7.0  65.0  48.0   37.0    5.0  2703.0   806.0    32.0   \n",
       "318  492.0  136.0    5.0  76.0  50.0   94.0   12.0  5511.0  1511.0    39.0   \n",
       "319  475.0  126.0    3.0  61.0  43.0   52.0    6.0  1700.0   433.0     7.0   \n",
       "320  573.0  144.0    9.0  85.0  60.0   78.0    8.0  3198.0   857.0    97.0   \n",
       "321  631.0  170.0    9.0  77.0  44.0   31.0   11.0  4908.0  1457.0    30.0   \n",
       "\n",
       "     CRuns   CRBI  CWalks  PutOuts  Assists  Errors  League_N  Division_W  \\\n",
       "1    321.0  414.0   375.0    632.0     43.0    10.0         1           1   \n",
       "2    224.0  266.0   263.0    880.0     82.0    14.0         0           1   \n",
       "3    828.0  838.0   354.0    200.0     11.0     3.0         1           0   \n",
       "4     48.0   46.0    33.0    805.0     40.0     4.0         1           0   \n",
       "5    501.0  336.0   194.0    282.0    421.0    25.0         0           1   \n",
       "..     ...    ...     ...      ...      ...     ...       ...         ...   \n",
       "317  379.0  311.0   138.0    325.0      9.0     3.0         1           0   \n",
       "318  897.0  451.0   875.0    313.0    381.0    20.0         0           0   \n",
       "319  217.0   93.0   146.0     37.0    113.0     7.0         0           1   \n",
       "320  470.0  420.0   332.0   1314.0    131.0    12.0         0           0   \n",
       "321  775.0  357.0   249.0    408.0      4.0     3.0         0           1   \n",
       "\n",
       "     NewLeague_N  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  \n",
       "5              0  \n",
       "..           ...  \n",
       "317            1  \n",
       "318            0  \n",
       "319            0  \n",
       "320            0  \n",
       "321            0  \n",
       "\n",
       "[263 rows x 19 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **++++ CRUCIAL NOTE ++++**\n",
    "**Regression** problemlerinde **,** öncelikle MUHAKKAK STANDARDİZASYON yapmamız gerekiyor...\n",
    "<br> **Classification** problemlerinde de STANDARDİZASYON yapılabilir... Ancak **Regression** problemlerinde KESİN yapılmalıdır..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Veri Setimizi TRAIN ve TEST olmak üzere ayıralım ve SONRA da STANDARDİZASYON işlemlerini gerçekleştirelim **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Split data into training and test sets :\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_trainStandard = scaler.transform(X_train)\n",
    "X_testTransformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **++++ CRUCIAL NOTE ++++**\n",
    "**Random Forest** konusunda da söylediğimiz gibi **;** En iyi Parametreleri bulmak için kullandığımız mesela aşağıdaki **'GradientBoostingRegressor'** tekniğine **,** yukarıdaki **Standardize Edilmiş** Veri Seti üzerinden **'Grid Search'** YAPMAMAMIZ LAZIM **!** Bu **Standardize Edilmiş** Veri Seti üzerinden **'Grid Search'** yaparsak **, VALIDATION Setini de görerek bir eğitim işlemi yapmış oluruz... Ve bu büyük bir hatadır...**\n",
    "<br> Bunun için de **; 'Grid Search'**'e başlarken **, STANDARDİZE EDİLMEMİŞ Veri Seti üzerinden işlemimizi gerçekleştiriyoruz...**\n",
    "\n",
    "AMA **PIPELINE** işleminde **;** ÖNCE STANDARDIZE Edip DAHA SONRA Tekniğimizi uyguluyoruz...\n",
    "\n",
    "**+--- NOTE --->** Aynı şekilde **,** bu işlemlerin hepsini **CLASSIFICATION** problemlerinde de uygulayabilirdik..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'Regressor__learning_rate': 0.01, 'Regressor__n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(max_depth=1)\n",
    "\n",
    "n_estimators = [100 , 1000 , 5000 , 10000]\n",
    "# Number of features at every split ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'Regressor__n_estimators' : n_estimators ,\n",
    " 'Regressor__learning_rate' : learning_rate ,\n",
    " }\n",
    "\n",
    "pipe = Pipeline([('scaler', preprocessing.StandardScaler()) , ('Regressor', model)])\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=pipe , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='neg_mean_squared_error' , n_jobs= -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **++++ CRUCIAL NOTE ++++**\n",
    "Yukarıda **, PIPELINE** içerisinde hem **'scaler'** hem de **'Regressor'** Teknikleri olduğu için **;** Sözlük türünde oluşturduğumuz Parametrelerin isimlerine direkt **\"'n_estimators'\"** ve **\"'learning_rate'\"** şeklinde yazamıyoruz... **PIPELINE** içindeki hangi Tekniğe AİT İSE **,** onun ismini de MUHAKKAK belirtmemiz gerekiyor...\n",
    "<br> **\"'Regressor__n_estimators'\"** ve **\"'Regressor__learning_rate'\"** ... gibi ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2477818958088657"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**++++ NOTE ++++**\n",
    "<br> Yukarıdaki En İyi Parametreleri bulma işleminde **,** en iyi Skoru **,** EKSİLİ bir değer bulmasının sebebi **, (scoring='neg_mean_squared_error')** metriğini kullanmamızdır... Normalde **,** direkt **'mean_squared_error'** Skoruna baktığımızda **,** metrik Skoru EN KÜÇÜK olan Parametreyi seçiyoruz... Ama burada **'neg_mean_squared_error'** metriğinin Skoruna baktığımız için **,** bu defa da metrik skoru EN BÜYÜK olan Parametreyi seçiyoruz... Çünkü sonuçları EKSİLİ şekilde veriyor... Ama tabii bu işlemleri bizim için **Scikit-Learn** yapıyor...\n",
    "<br> İstersek bu ortaya çıkan BEST SCORE'yi tekrar EKSİ ile çarpıp **,** ARTI hale getirebiliriz ama gerek yok... Biz zaten sadece En İyi Parametreyi arıyoruz...\n",
    "<br> Yani **Cross Validation** değeri en iyi olan Parametrenin Skoru **'-0.2477818958088657'** imiş...\n",
    "\n",
    "Bütün Parametre Skore'lerini görmek istersek de **, ``'boost_grid.cv_results_'``** komutunu yazabiliriz..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Şimdi de **,** en iyi parametreleri kullanarak EN BAŞTAN modelimizi kuralım **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.17972168739765562\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=1000 , max_depth=1 , learning_rate=0.01)\n",
    "model.fit(X_trainStandard , y_train)\n",
    "y_pred = model.predict(X_testTransformed)\n",
    "print('Mean Squared Error:' , mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **++++ CRUCIAL NOTE ++++**\n",
    "Yukarıda gördüğümüz gibi **,** En İyi Parametreleri seçtikten sonra **,** modeli EN BAŞTAN kurarken ve Tahminleme yaparken işlemlerimizi **,** en başta ki **Standardize Edilmiş** Veri Seti üzerinden yapıyoruz..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011813996070491"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **``Regression``** with ``Adaptive Boosting``\n",
    "### **'AdaboostRegressor'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'Regressor__learning_rate': 1, 'Regressor__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model = AdaBoostRegressor()\n",
    "\n",
    "n_estimators = [100 , 1000 , 5000 , 10000]\n",
    "# Number of features at every split ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'Regressor__n_estimators' : n_estimators ,\n",
    " 'Regressor__learning_rate' : learning_rate ,\n",
    " }\n",
    "\n",
    "pipe = Pipeline([('scaler', preprocessing.StandardScaler()) , ('Regressor', model)])\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=pipe , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='neg_mean_squared_error' , n_jobs = -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.1949846603112319\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostRegressor(n_estimators=100 , learning_rate= 1)\n",
    "model.fit(X_trainStandard , y_train)\n",
    "y_pred = model.predict(X_testTransformed)\n",
    "print('Mean Squared Error:' , mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **``Regression``** with ``Extreme Gradient Booster`` (**XGBoostRegressor**)\n",
    "### **'XGBRegressor'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "{'Regressor__learning_rate': 0.01, 'Regressor__n_estimators': 5000}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "n_estimators = [100 , 1000 , 5000 , 10000]\n",
    "# Number of features at every split ::\n",
    "learning_rate = [1 , 0.1 , 0.01 , 0.001]\n",
    "# Create grid ::\n",
    "params = {\n",
    " 'Regressor__n_estimators' : n_estimators ,\n",
    " 'Regressor__learning_rate' : learning_rate ,\n",
    " }\n",
    "\n",
    "pipe = Pipeline([('scaler', preprocessing.StandardScaler()) , ('Regressor', model)])\n",
    "# Random search of parameters ::\n",
    "boost_grid = GridSearchCV(estimator=pipe , param_grid=params , \n",
    "                          cv=5 , verbose=2 , scoring='neg_mean_squared_error' , n_jobs = -1)\n",
    "# Fit the model ::\n",
    "boost_grid.fit(X_train , y_train)\n",
    "# Print results ::\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.193993561571042\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(n_estimators=5000 , learning_rate=0.01)\n",
    "model.fit(X_trainStandard , y_train)\n",
    "y_pred = model.predict(X_testTransformed)\n",
    "print('Mean Squared Error:' , mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01663682, 0.00632886, 0.00481615, 0.01029918, 0.00993644,\n",
       "       0.01325134, 0.02696959, 0.23648155, 0.36681116, 0.06597257,\n",
       "       0.08040313, 0.08907861, 0.03242335, 0.01212592, 0.00587025,\n",
       "       0.0079202 , 0.00609087, 0.00444641, 0.00413751], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``'feature_importances_'``** komutu **;** HER BİR FEATURE'nin ne kadar **'IMPORTANT (ÖNEMLİ)'** olduğunu gösterir...\n",
    "<br> En iyi model kurulduktan sonra **,** sonuç olarak çıkan bu değerler **ne kadar büyük ise metrik sonucu o kadar küçülmüş demektir...** Yani iyileşmiş demektir... Onun için de en önemli Feature **,** EN ÇOK **improvement** olan **,** yani iyileşme gösteren Feature'dir..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
